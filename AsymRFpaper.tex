\documentclass[twoside,11pt]{article}
\usepackage{sty_files/jmlr2e}
\usepackage{amsmath,multirow,subfigure,bigstrut}
\usepackage[ruled,vlined]{algorithm2e}	
% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}
\jmlrheading{1}{2012}{1-48}{4/00}{10/00}{George Runger and Derek Koh}

% Short headings should be running head and authors last names
\ShortHeadings{Asymmetric Random Forest for Variable Selection}{Runger and Koh}
\firstpageno{1}

\title{Asymmetric Random Forest for Variable Selection}

\author{\name George Runger \email George.Runger@asu.edu\\
       \addr Sch Compt Infor \& Dec Sys Engr\\
       Arizona State University\\
       Tempe, AZ 85281, USA
       \AND
       \name Derek Koh \email Derek.Koh@asu.edu \\
       \addr Sch Compt Infor \& Dec Sys Engr\\
       Arizona State University\\
       Tempe, AZ 85281, USA}

\editor{Leslie Pack Kaelbling}


\begin{document}
\maketitle

\section{Introduction}
In Chapter \ref{chapter:SVMpaper}, an asymmetric loss function was proposed and applied on SVM. Some attempts were made to create ensembles out of this SVM to improve accuracy. The focus for this chapter is on picking the variables that are relevant for asymmetric prediction. The benefits for asymmetric learners as oppose to symmetric learners have been heavily made in Chapter \ref{chapter:SVMpaper}. It is only natural to explore a better way of variable selection such that the right variables are used in the classifier to produce high accuracy for the class of interest. Trees traditionally take the mode or average value of the leaf node to classify an observation that falls into that leaf. Such central tendency predictions are not robust to asymmetric predictions because they fail to account for the risk of misclassification of one side of a binary class. Tree based algorithms can be modified, like the SVM, such that it will be able to make asymmetric predictions. Similarly, the generic random forest algorithm, which has a symmetric loss function, can be modified to allow for asymmetry

\section{Background}
Variable selection is an important part of machine learning as it provides a subset of variables that are useful for prediction. Predictors selected by variable selection techniques have the ability to predict the values of predictors with little consideration for the ability to predict a particular value of the predictor better. As asymmetric problems are concerned with the problem of predicting one response with higher precision, one can revamp the focus of variable selection such that we choose variables that are most capable in predicting a specified response value.

In asymmetry, we do not need the variable to be apt at predicting all values of the response, only the specific value that we want to focus on. Let $y \in (-1,1)$, we have a model $f(\mathbf{x})=y$ where $\mathbf{x} = (x_1,x_2,...,x_p)$ and $p \in P$. We would like to find a set of $x$ where $x \in \mathbf{x} \in \mathcal{X}$ such that $\displaystyle\max_{\mathbf{x}}P(y=1,f(\mathbf{x})=1)$. Finding variables that maximizes the function $\displaystyle\max_{\mathbf{x}}P(y=f(\mathbf{x}))$ has been widely studied in the literature and is the basis for most variable selection algorithm. 

There are three main ways the variable selection is conducted \citep{Saeys07} : filter, wrapper, embedded. The filter selection techniques assesses the intrinsic properties of the predictor and assigns a score. The variables with the best scores are the picked. The simplest filters come in the form of t-stat or ANOVA  \citep{Jafari06}. Correlation based feature selection (CFS) is a method that picks variables with high correlation to the dependent variable and low correlation to the other independent variables \citep{Hall99}. Similar to CFS is the minimum Redundancy Maximum Relevance Feature Selection (mRMR) which is based on the concept of gathering a subset of independent variables that are highly predictive of the dependent variable and highly dissimilar with other independent variables \citep{Ding03}. Markov blanket filter builds a Markov blanket that contains a minimal subset of relevant features that yields optimal classification \citep{Zeng09}. 

Wrappers utilize a learning algorithm as a black box to score subsets of variable according to their predictive power. The wrapper methodology was popularized by \citet{Kohavi96} and is a simple and powerful way to address the problem of variable selection, regardless of the chosen algorithm. Some popular known wrapper algorithms include sequential forward and backward selection \citep{Kittler78}, zimulated annealing\citep{Kirkpatrick83}, randomized hill climbing \citep{Skalak94}, genetic algorithms \citep{Holland75}, and estimation of distribution algorithms (EDA) \citep{Blanco04}. Sequential forward and backward selection uses regression as the learner to decide the best subset of variables that are relevant. Simulated annealing, randomized hill climbing and genetic algorithms both use some form of randomness to find a subset of variables that are pertinent for model prediction. The estimation of distribution algorithm is a general version of the genetic algorithm. When iterating a genetic algorithm to find the best subsets, equal priors are placed on each of the variables in the current subset. In EDA, the priors are updated and represents a probability distribution of the variable's pertinence. 

For embedded methods, the search for a best subset of features is built into the classifier construction, and can be seen as a search in the combined space of feature subsets and hypotheses. This process may be more efficient in several respects. First, it makes use of the available data by not needing to split the training data into a training and validation set. Second, it reaches a solution faster by avoiding retraining a predictor from scratch for every variable subset investigated. Embedded methods are not new: decision trees such as CART \citep{Breiman84}, for instance, have a built-in mechanism to perform variable selection. Random forest is an algorithm that is widely used in the literature as an embedded system for variable selection \citep{Diaz06}\citep{Jiang04}. A variant of SVM known as SVM with recursive feature selection is an SVM method that has an embedded feature selection technique \citep{Guyon02}. Finally \citet{Ma05} tweaks the logistic regression method to produce an embedded method that eliminates variables with small weights.

A review of the methods in the literature failed to shed light on finding variables that are particularly good at asymmetric prediction. Asymmetric variables appear to be an area that has not been widely studied. However, asymmetric variables appear to be prevalent in the literature. \citet{Matzler04} did a regression analysis to find out what attributes lead to better overall customer satisfaction. They showed that for variables of complaint handling, project management, and innovativeness, when the performance is low, there is a significant effect on overall satisfaction. However, when these same variables showed high performance, the effect on overall satisfaction is insignificant. \citet{Froyen97} studied whether political pressures add significant explanatory power in monetary policy. They found that in white house administrations that pressure the Federal Reserve to tighten monetary policy results in higher interest rates whereas in white house administrations that promote loose monetary policy does not affect the interest rates. Finally \citet{Karras97} identified money-supply shocks and their effects on output for a panel of 18 European countries and found that many different specifications and estimation methods strongly support asymmetry: negative money-supply shocks are shown to have a statistically significant effect on output, whereas the effect of positive shocks is statistically insignificant. 

\section{Loss functions in trees}
For this research, we will be focusing on modifying the random forest algorithm by \citet{Breiman01}. But before we discuss how the random forest algorithm works, it is worth while to discuss the precursor to the random forest algorithm. We will be looking specifically at the CART algorithm by \citet{Breiman84}. In general, the algorithm by \citet{Breiman84} proposed divides the predictor space by recursively splitting the data. Each splitting rule splits the space into two separate regions $R_i$ and $R_j$, $i \neq j$. Each region $R_m,m \in M$ is called a node. Each rule forms a decision boundary, and a leaf node is the final partition after all the rules are enforced. The classification of data is done at the leaf node by the following loss function for a regression tree

\begin{equation}
	\begin{array}{rcl}
		\min_{\gamma_j} \sum_{i=1}^N L(y_i,\gamma_j) & = &  \min_{\gamma_j} \sum_{i=1}^N (y-\gamma_j)^2 \\
	\mbox{where }	\gamma_j &\in & \mathbb{R}	
	\end{array}
\end{equation}

and 
\begin{equation}
	\begin{array}{rcl}
		 \min_{\gamma_j} \sum_{i=1}^N L(y_i,\gamma_j) & = &  \min_{\gamma_j} \sum_{i=1}^N 1-I(y=\gamma_j) \\
	\mbox{where }	\gamma_j &\in & Y
	\end{array}
\end{equation}

 
for a classification tree\footnote{Dr Runger: please note that the functions shown apply only to the leaf node and is not used for constructing the tree}. It can be shown that the value of $\gamma$ is $E_{x_i \in R_j}[y]$ for a regression tree and $\arg\max_{y\in Y} y$ for a classification tree. The rules for creating the partitions of the feature space is done in the following fashion. For all observations that fall into region $R_m$, an optimal split is found by finding that split that results in the greatest purity. The purity is calculated with a purity function which in the case of CART is the Gini index. Let $k \in K$ be a class of the dependant variable and $p_{mk}$ be the probability of class $k$ in node $R_m$, then the Gini is as follows.

\begin{equation}
	G(m)= 1-\displaystyle\sum_k p_{mk}
\end{equation}

Every possible split within the region $R_m$ is considered in finding the optimal split. Splitting of regions continue until no split that increases purity can be made. Finding optimal regions in this fashion often results in a local optimum as the splitting decision is greedy. Thus a way to manage this problem is by using more than one tree and aggregating the prediction of all the trees. A method that does just that is the random forest algorithm

\section{Random Forest}
Random forest was created by \citet{Breiman01}. The method grows $N-$number of trees and takes the vote of all the trees grown to make a prediction. The trees are grown with the use of the CART algorithm with one exception: A random subset of the total number of variables instead of all the variables is searched on to find the greatest change in purity. The selection process of using a random subset of the total number of variables is used in calculating the Gini index mediates the problem of obtaining a local minimum that the CART algorithm suffers from. The below algorithm shows how a random forest is constructed

\begin{algorithm}
Let the number of training cases be $N$, and the number of variables in the classifier be $M$.

\For{each tree}{
Obtain a bootstrap sample of the training data of size N.\\
Pick m variables at random from M for use to determine the best split at each node of the tree.
}
Make a prediction by letting each tree classify an observation and then take the majority vote as the prediction.
\caption{Random Forest Algorithm}\label{algo: RandomForest}
\end{algorithm}

\subsection{Random Forest for variable selection}
\citet{Breiman01} showed that there are two ways the random forest algorithm can be modified to be used for variable selection. The first way is by random permutation. For each tree grown in the forest, classify the out-of-bag (OOB) observations and take the accuracy of the results. Then, for each predictor $x$, permute the variable values and reclassify the OOB observations. Compare the results to the initial accuracy. The most important variables are the ones that result in the greatest drop in accuracy

When a predictor is split on the Gini index, the information gain $\mathcal{IG}(\cdot)$ is calculated as follows. 

\begin{equation}\label{eqn:Delta Function}
	\mathcal{IG}(m)=G(m)-\rho^L G(m^L)-\rho^R G(m^R) 
\end{equation}
\begin{equation}
	\text{where } \rho^L = \frac{|m^L|}{|m|} \text{ and } \rho^R = \frac{|m^R|}{|m|}
\end{equation}
	



$m^L$ and $m^R$ are the left and right split of the region $R_m$. 

The second method for variable selection uses this purity to get the most relevant variables. For each of the $x_i$'s, The overall purity is calculated by summing the purity gained from each split that was split by $x_i$. The most important variable are then the ones with the greatest purity. For the work done in this paper, we will focus on variable importance using the Gini purity method. 	

\section{Asymmetric Variable Selection}
For binary classifying problems where $y\in (-,+)$, variable selection such as the random forest variable selection methods will select variables that are apt at predicting both the positive and negative class. No consideration is given to the possibility that certain variables are better at predicting one side than the other. For instance, \cite{Bettis05} showed that exercising of employee stock options early is a sign for poor company performance but late exercise does not indicate superior performance. Thus earliness of option exercise cannot be used to predict if a firm will out-perform but earliness of exercise can be used to predict underperformance. If we are only interested in the results on one side, it would be better to have a variable selection algorithm that selects variables that are predictive on that side. 

We need a selection technique that modifies the tree algorithm such that there is a bias towards splits that result in nodes that are pure in class +. This requires asymmetric treatment when splits result in nodes that are predicted - and nodes that are predicted +. From equation \ref{eqn:Delta Function}, we see that $\mathcal{IG}(\cdot)$ depends on the impurity function $G$, and $\rho$ which is the weight given to the $G$'s. Asymmetry is implemented by changing the purity function G such that it has a bias to one class. We can also achieve asymmetry by adjusting $\rho$ to be greater when the resulting class is +. 

For a binary problem, the Gini index is as follows
\begin{equation}\label{eqn:giniindex}
	G(m)=1-p_{m(+)}^2-(1-p_{m(+)})^2
\end{equation}

We should like to modify the Gini as follows such that the resulting tree is more susceptable to splits that results in an increase in purity for class +. 

\begin{equation}\label{eqn:asymmetricginiindex1}
	aG1(m)=\min(1-p_{m(+)}^2-(1-p_{m(+)})^2, 1-p_{m(-)})
\end{equation}

From the equation, if class + is the majority class, then the $aG1(m)$ will resemble a regular Gini index. Otherwise  $aG1(m)$ will be a straight line. Figure \ref{Fig:Quantile Regression} shows the graph of both the regular Gini and the asymmetric Gini for a binary response variable.

\begin{figure}
 \centering
\includegraphics[width=3.4in]{img/asymRFimages/AsymmetricGini}\\
 \caption{Impurity for different values of p.}
 \label{Fig:Quantile Regression}
\end{figure}

We want the impurity change to be strictly convex when the majorty class from the split is + and linear when the majority class is -. This ensures that for splits that result in the same increase of proportion for either class + or -, the $\mathcal{IG}$ when class + is the majority class will be higher than when class - is the majority class. We will then be able to grow trees that have leafs with higher purity for the + class and as the variable selection algorithm sums up all $\mathcal{IG}$ caused by node splits from trees, a variable that is more capable at predicting class + will more likely have a higher score. 

We can take the idea of $aG1(m)$ a step further and ignore the impurity when the class is -1. Equation \ref{Eq:asymmetricginiindex2} shows how such a impurity function, $aG2(m)$ is calculated

\begin{equation}\label{Eq:asymmetricginiindex2}
aG2(m)=
\begin{cases} 1-p_{+}^2-(1-p_{+})^2 & \text{ if} \operatorname{arg\,max} y_m = + 
\\ G(m_{parent})&\text{ otherwise}
\end{cases}
\end{equation}

High $aG1(m)$ or $aG2(m)$ often times result in low $|m|$. This ultimately results in low $\rho$ that are associated with $m^{(+)}$ and high $\rho^{(+)}$ that are associated with the complement of $m^{(+)}$. A solution to lower the magnitude different $\rho^{(+)}$  and $\rho^{(-)}$ would be to log the values of $|m|$ as follows
\begin{equation}\label{eqn:rho_log}
	\rho_{lg}^L = \frac{log(|m^L|)}{log(|m^L|)+log(|m^R|)}
\end{equation}
\begin{equation}
	\rho_{lg}^R = \frac{log(|m^R|)}{log(|m^L|)+log(|m^R|)}
\end{equation}


Finally, another solution would be to square root the cardinality of the node that is classified as -. This minimizes the impact of the information gain that is attributed to a child node that is classified as -. Equation \ref{eqn:rho_sqrt} shows this.

 \begin{equation}\label{eqn:rho_sqrt}
	\rho_{sqrt}^L = \frac{|m^{(+)}|}{|m^{(+)}|+|m^{(-)}|}
\end{equation}
\begin{equation}
	\rho_{sqrt}^R = 1- \rho_{sqrt}^L
\end{equation}


\section{Methodology}
To pick variables, the asymmetric random forest algorithm is as follows.

\begin{algorithm}
Let the number of training cases be $N$, and the number of variables in the classifier be $M$.
\For{each tree}{
Obtain a bootstrap sample of the training data of size $N$.\\
Pick $m$ variables at random from $M$ for use to determine the best split at each node of the tree.\\
	\While{$\mathcal{IG}(\cdot) >0$ }{
		\For{each m}{
			calulate $\mathcal{IG}(m)$
		}
		pick the $m$ with the highest $\mathcal{IG}$ as the splitting node\\
		Split into 2 sub nodes.\\
		Repeat\\
	}
}
\For{each variable}{
	\For{each tree}{
		\For{each node}{
			\If{$m$ is the node used for splitting}{
				tally the $\mathcal{IG}(m)$
			}
		}
	}
}
Rank the tallied variables in descending order 

\caption{Asymmetric Random Forest Variable Selection}\label{algo: AsymRandomForest}
\end{algorithm}

\section{Experiment}
\subsection{Asymmetric Forest on Simulated Asymmetetric Features}
In an effort to discover asymmetric variables we simulated features with varying precision. The features are binary variables and the response is coded as - and + to indicate the negative and positive class. In total we simulated 30 features: 1 symmetric feature, 6 asymmetric features with high precision on class + and 6 asymmetric features with high precision on class -. Feature 1 has high precision for both the positive and negative class and is most predictive. Feature 2 to 7 have precision for class + higher than that of class - and feature 8 to 13 has the converse. The rest of the features are noise variables. In an ideal sitation, the asymmetric random forest will pick only the variables that have high precision in the positive class (feature 1 to feature 7). Table \ref{tab:asymmetricfeatures1} shows these features and their precisions for each of the classes. 

% Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{table}[htbp]
  \centering
  \caption{Add caption}
    \begin{tabular}{rrr}
    \hline
    Feature & Precision (Class +) & Precision (Class -) \bigstrut\\
    \hline
    1     & 0.9   & 0.89 \bigstrut[t]\\
    2     & 0.83  & 0.53 \\
    3     & 0.84  & 0.64 \\
    4     & 0.72  & 0.62 \\
    5     & 0.75  & 0.66 \\
    6     & 0.63  & 0.59 \\
    7     & 0.56  & 0.55 \\
    8     & 0.53  & 0.82 \\
    9     & 0.65  & 0.84 \\
    10    & 0.61  & 0.7 \\
    11    & 0.66  & 0.75 \\
    12    & 0.6   & 0.64 \\
    13    & 0.54  & 0.55 \\
    14-30 & 0.5   & 0.5 \bigstrut[b]\\
    \hline
    \end{tabular}%
  \label{tab:asymmetricfeatures1}%
\end{table}%

5000 observations were simulated. Five draws of 1000 samples each were drawn from the 5000 observations for use in the Asymmetric Random Forest. We run the Asymmetric Random Forest for each of these draws using various combinations of the Gini, $\rho$, and number of trees in the forest. The average ranks for features 1 to 7 was calculated. We subtracted the average rank from that of the average ranks of features 8 to 13. Finally we subtracked the average rank of features 1 to 7 from the average rank of the noise features. The results are shown in table \ref{tab:asymmetricfeaturesresults}.


% Table generated by Excel2LaTeX from sheet 'Sheet2'

\begin{table}%[htbp]
  \centering
  \caption{Results from simulated asymmetric variables. Average feature rank is the average rank of class + asymmetric features (feature 2-7). Difference between complement features is the difference of the average rank of features 2-7 from features 8-13. Diff between noise features is the difference of the average rank of features 2-7 from features 14-30}
\begin{tabular}{rrrp{2.5cm}p{2.5cm}p{2.5cm}}
\hline
Gini Type & P Type & No. of Trees & Average Feature Rank & Diff between Complement Features & Diff between Noise Features \bigstrut\\
\hline
\renewcommand{\arraystretch}{.5}
aG1   & $\rho$ & 50    & 13.60 & 0.00  & 4.10 \bigstrut[t]\\
aG1   & $\rho$ & 100   & 14.53 & -2.53 & 3.05 \\
aG1   & $\rho$ & 200   & 12.60 & 0.60  & 5.76 \\
aG1   & $\rho_{lg}$ & 50    & 11.47 & 0.80  & 7.72 \\
aG1   & $\rho_{lg}$ & 100   & 11.53 & 0.67  & 7.40 \\
aG1   & $\rho_{lg}$ & 200   & 10.83 & 1.77  & 8.25 \\
aG1   & $\rho_{sqrt}$ & 50    & 19.20 & -4.90 & -3.76 \\
aG1   & $\rho_{sqrt}$ & 100   & 20.03 & -4.60 & -5.68 \\
aG1   & $\rho_{sqrt}$ & 200   & 20.27 & -6.30 & -5.09 \\
aG2   & $\rho$ & 50    & 7.80  & 9.20  & 10.83 \\
aG2   & $\rho$ & 100   & 8.40  & 5.70  & 10.96 \\
aG2   & $\rho$ & 200   & 7.43  & 8.37  & 11.53 \\
aG2   & $\rho_{lg}$ & 50    & 8.37  & 3.77  & 11.90 \\
aG2   & $\rho_{lg}$ & 100   & 6.43  & 4.20  & 14.90 \\
aG2   & $\rho_{lg}$ & 200   & 6.57  & 4.67  & 14.43 \\
aG2   & $\rho_{sqrt}$ & 50    & 7.70  & 5.47  & 12.35 \\
aG2   & $\rho_{sqrt}$ & 100   & 7.97  & 5.20  & 11.90 \\
aG2   & $\rho_{sqrt}$ & 200   & 5.80  & 6.20  & 15.53 \\
G     & $\rho$ & 50    & 12.83 & 2.63  & 4.64 \\
G     & $\rho$ & 100   & 14.70 & -0.37 & 2.30 \\
G     & $\rho$ & 200   & 14.83 & -1.57 & 2.65 \\
G     & $\rho_{lg}$ & 50    & 15.20 & -2.30 & 1.89 \\
G     & $\rho_{lg}$ & 100   & 13.47 & -1.03 & 4.71 \\
G     & $\rho_{lg}$ & 200   & 13.20 & -2.47 & 5.60 \\
G     & $\rho_{sqrt}$ & 50    & 15.73 & -1.03 & 0.57 \\
G     & $\rho_{sqrt}$ & 100   & 16.93 & -1.57 & -1.17 \\
G     & $\rho_{sqrt}$ & 200   & 15.93 & -0.67 & 0.12 \bigstrut[b]\\
\hline
\end{tabular}%
\label{tab:asymmetricfeaturesresults}%
\end{table}%


\subsection{Asymmetric Forest on Simulated Asymmetric features that interacts}
Asymmetric precision in predictors can happen when there is interaction between multiple predictors. In the following experiment, we deconstructed asymmetric predictors that has high prediction for class + into 2 new predictors that have a predicision of 50\% for class + and class -. The deconstruction for each predictor is done by algorithm \ref{algo: PredictorDecompose}. We used two features that have high precision in class + and two features that have high precision in class - and decomposed them to eight features that are individually unpredictive. We also added four noise variables to the data for simulation. The asymmetric forest algorithm with various Gini's and $\rho$s were ran. The results are as table \ref{tab:asymmetricfeaturesresults2}. 
\begin{algorithm}
Let variable $x_c\in X$ where $X$ is a binary variable be the value that results in the highest precision for class $c$.\\
To decompose $x$ into $z_a$ and $z_b$
\If{$x==x_c$}{
	Generate random number $R$
	\If{$R<.5$}{
		$z_a=1$\\
		$z_b=0$
	}
	\Else{
		$z_a=0$\\
		$z_b=1$
	}
}
\Else{
	Generate random number $R$
	\If{$R<.5$}{
		$z_a=1$\\
		$z_b=1$
	}
	\Else{
		$z_a=0$\\
		$z_b=0$
	}
}

\caption{Decomposing predictors algorithm}\label{algo: PredictorDecompose}
\end{algorithm}


\begin{table}%[htbp]
  \centering
  \caption{Results from simulated asymmetric variables with interactions. Average feature rank is the average rank of class + asymmetric features. Difference between complement features is the difference of the average rank of class + features from class - features. Diff between noise features is the difference of the average rank of class + features from the noise features}
\begin{tabular}{rrrp{2.5cm}p{2.5cm}p{2.5cm}}
\hline
Gini Type & P Type & No. of Trees & Average Feature Rank & Diff between Complement Features & Diff between Noise Features \bigstrut\\
\hline
\renewcommand{\arraystretch}{.5}
aG1   & $\rho$ & 50    & 7.67  & 1.25  & 1.25 \bigstrut[t]\\
aG1   & $\rho$ & 100   & 7.17  & 3.08  & 0.91 \\
aG1   & $\rho$ & 200   & 8.08  & 0.92  & 0.33 \\
aG1   & $\rho_{lg}$ & 50    & 6.75  & 3.58  & 1.67 \\
aG1   & $\rho_{lg}$ & 100   & 6.25  & 4.17  & 2.58 \\
aG1   & $\rho_{lg}$ & 200   & 6.09  & 3.42  & 3.84 \\
aG1   & $\rho_{sqrt}$ & 50    & 5.34  & 5.33  & 4.17 \\
aG1   & $\rho_{sqrt}$ & 100   & 5.42  & 5.67  & 3.59 \\
aG1   & $\rho_{sqrt}$ & 200   & 4.50  & 6.92  & 5.09 \\
aG2   & $\rho$ & 50    & 7.08  & 3.17  & 1.08 \\
aG2   & $\rho$ & 100   & 6.42  & 3.00  & 3.08 \\
aG2   & $\rho$ & 200   & 6.08  & 3.84  & 3.42 \\
aG2   & $\rho_{lg}$ & 50    & 5.58  & 3.25  & 5.33 \\
aG2   & $\rho_{lg}$ & 100   & 5.75  & 2.58  & 5.66 \\
aG2   & $\rho_{lg}$ & 200   & 7.00  & 0.67  & 3.83 \\
aG2   & $\rho_{sqrt}$ & 50    & 5.92  & 4.09  & 3.50 \\
aG2   & $\rho_{sqrt}$ & 100   & 5.42  & 3.00  & 6.25 \\
aG2   & $\rho_{sqrt}$ & 200   & 4.59  & 4.66  & 7.08 \\
G     & $\rho$ & 50    & 8.42  & 0.25  & 0.00 \\
G     & $\rho$ & 100   & 8.75  & -0.50 & -0.25 \\
G     & $\rho$ & 200   & 9.17  & -1.25 & -0.75 \\
G     & $\rho_{lg}$ & 50    & 8.34  & -0.59 & 1.08 \\
G     & $\rho_{lg}$ & 100   & 7.58  & 1.17  & 1.58 \\
G     & $\rho_{lg}$ & 200   & 8.67  & -0.33 & -0.16 \\
G     & $\rho_{sqrt}$ & 50    & 6.58  & 2.50  & 3.25 \\
G     & $\rho_{sqrt}$ & 100   & 6.42  & 3.00  & 3.25 \\
G     & $\rho_{sqrt}$ & 200   & 6.34  & 3.50  & 3.00 \bigstrut[b]\\
\hline
\end{tabular}%
\label{tab:asymmetricfeaturesresults2}%
\end{table}%


\subsection{Asymmetric features in Form 4 data}
Public companies that trade in the United States must have its directors, officers, and sharesholers owning more than 10\% of the firm file with the United States Securities and Exchange Commission a statement of ownership anytime they conduct a transaction with a company's stock. The filing is done by the individual trading the security with the form 4 document and the filing is made available by the SEC on its website \ref{SEC WEBSITE ???}. Based on the research done by \ref{Cicero09}, executive trade on privileged information and early exercise is correlated with the peak of a stock price run. However, late exercise does not correlate with the bottom of a stock price run. We picked 20 variables that are associated with executive stock trading and ran the asymmetric random forest algorithm. The results are as below.  



\section{Conclusion}


\bibliography{bibliography}
\end{document}