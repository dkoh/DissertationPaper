\chapter{Introduction}
\section{Introduction}
In an era of information technology there is an increasing amount of data that is becoming readily available. The types of data and the magnitude of data sets are getting larger and there is a need to make sense of such information. Such large data sets require the use of new tools and technologies to process and learn information present within the data. The field of statistical and machine learning is thus becoming more and more important in the research of such quantities of data available. Designed to overcome the short comings of the older statistical methods, these new tools present an improvement over the rudimentary methods such as regression and ANOVA, optimizing knowledge discovery in a large data, higher-dimensional, and non-linear context. The use of many of these advance tools bring the need for research in the drivers of the algorithms and how to the improve upon them. It is commonly thought of that increasing the sophistication of learning inevitably increases the risk of obscuring what is actually important in the discovery, also known as overfitting. This has been shown in many practical applications where increasing the use of sophisticated methods without regard for context results in poor performance. Thus research is important in the area of fine tuning such novel methods in the field so as to ensure the right inference is obtained from the data. 

The work done in this thesis focuses on risk reduction in prediction models. A desired outcome for models is to get high accuracy in prediction. In a binary true and false prediction, accuracy comes from either predicting a true-positive or a true-negative. In this research, we decompose such accuracy into either the true-positive or true-negative and optimize the parts separately instead of together. We focus on a unique type of fine tuning where data is analyzed asymmetrically. 

The thesis starts off with researching this method of accuracy optimization by analyzing problems that are only focused on the results on one side. An example of such may be a one-sided tail test in statistics or learning from a data set that is unbalanced. For such problems, an asymmetric method of analyzing data will prove to be more effective as it directly addresses the problem. For instance, suppose a fund manager tells an investor that he will expect to get 10\% return per year if he invests his money in the fund. After a year, the fund will either beat or miss the target. For the investor, if the fund beats the target, he or she will not be unhappy with what was promised by the fund manager. However,
if the fund were to miss the target, the investor would be unhappy. Thus in this situation, it would be more important to ensure that the realized returns does not fall below the promised target than to ensure that the realized return exceed the promised target. Most learners such as regression uses the squared error loss that is indifferent towards whether or not the error is above or below the target. As we are only concern with the asymmetric nature of the outcome, modifying the existing complex algorithms to learn asymmetrically would ensure that such problems are directly addressed and resources are not wasted on the side that is not important. After such asymmetric optimization is studied it would then be possible to form different learners that optimizes either side of the error and combine them to get a symmetric learner. We would expect that this symmetric learner would then have better accuracy as oppose to a model that learns both sides of the error at the same time. This research contributes to the work done in the current literature in three parts. 

The first part of this research presents a generalized way of looking at asymmetric loss. As there are already existing functions that addresses such problems, this part contributes via presenting a general way of thinking about asymmetric loss and applies it to the support vector machine algorithm. To bridge the theoretical and the practical, this section also attempts to reconcile how such a loss function would be useful to the general community by presenting certain ways in which such functions can be utilized in practice. 

The next part strives to take the asymmetric ideas presented in the former research by exploring the power of variables that are only predictive in one direction. Variable selection has traditionally been focused on searching for variables that are predictive for all values of the response. For a binary classification problem, a variable that is predictive for both the positive and negative class is more desirable than that which is only able to predict the positive class when traditional variable selection techniques are applied. The second part of the research focuses on finding variables that are predictive in one direction, ignoring the ability of the variable to predict the other direction. This allows variables that are only predictive in one direction to be selected when in traditional variable selection algorithms, such variables will never be selected. The random forest algorithm not only has the ability of a classifier, it also is able to seek out the best variables in the data for prediction. A modification in the Gini index  for trees in a random forest is made to try and achieve asymmetric variable prediction. The Gini is forced to be asymmetric. The variable selection method for the random forest is then used to seek out variables that are predictive in the positive class. 

The third contribution of the research focuses on using models to manage non-linear 1-to-1 matched data. In conditional logistic regression, the data space is assumed to be homogenous in that the relationship between the case and the control is assumed to be invariant over all values of cases and control. Let $\mathbf{x}_0,\mathbf{x}_1$ be vectors of the control and case respectively, and $\mathbf{\beta}$ be the vector of the relationship between  $\mathbf{x}_0$ and $\mathbf{x}_1$. If the relationship between $\mathbf{x}_0$ and $\mathbf{x}_1$ is linear, then it follows that 
\begin{equation}\label{eqn:linearrelationship}
	E[\mathbf{\beta}|\mathbf{x}_0,\mathbf{x}_1]=E[\mathbf{\beta}]\\
\end{equation}

Conditional logistic regression assumes this linearity and hence cannot be used when the relationship between case and control is dependent on the attributes of the case or control. Specifically, if the vector $\mathbf{\beta}$ is a function of $\mathbf{x}_0$ and $\mathbf{x}_1$ as follows. 

\begin{equation}\label{eqn:nonlinearrelationship}
	E[\mathbf{\beta}|\mathbf{x}_0,\mathbf{x}_1]=f(\mathbf{x}_0,\mathbf{x}_1)
\end{equation}

 The problem of addressing such non-linearity in conditional logistic regression has not been addressed in the literature. Thus the third contribution uses models to tackle problems where the relationship between case and control follows equation \ref{eqn:nonlinearrelationship}. More specifically, we assume that there are distinct regions, $r\in\mathbb{R}$ where the relationship between case and control is linear as shown in equation \ref{eqn:conditionallinearrelationship}.

\begin{equation}\label{eqn:conditionallinearrelationship}
	E[\mathbf{\beta}|\mathbf{x}_0,\mathbf{x}_1]=E[\mathbf{\beta}_r]\\
\end{equation}
where $\mathbf{\beta}_r$ is the vector that represents the relationship when $\mathbf{x}_0,\mathbf{x}_1 \in r$. 


 We call these sub spaces stratums and propose applying a modified conditional logistic regression algorithm to discover the stratums where the relationship between case and control is linear. Within these stratums we can then apply linear models to determine the relationship between case and control. 

This proposal includes problem statement, literature review, and the proposed methods for the aims presented above with some results.

\section{Research Scope}
The research scope provides an overview of the research to be done for the dissertation, its contribution to the existing literature, and its importance in real world applications. The research done will consist of three parts spread out into three different chapters. The chapters builds on each other successively to define the overall thesis on asymmetric loss. 

\subsection{Asymmetric Support Vector Machines (Chapter~\ref{chapter:SVMpaper})}
In Chapter~\ref{chapter:SVMpaper}, a generalized asymmetric loss function is proposed and explored. There has been various studies of asymmetric loss in the literature. No one study has adequately articulated the full benefits of asymmetric loss in a general fashion. This study contributes to the existing literature by formally presenting the impetus for the use of such functions. It also extends the existing methods to form a generalized function for asymmetric loss. 

The two main problems that this study argues would be better with asymmetric loss are the class imbalance problem, and the one sided optimization problem. The chapter dwells into the existing literature where these problems are seen and makes an argument that improvements can be made to the results if the papers considered using asymmetric loss. In this chapter, a discussion of the concept of a loss function is introduced. Certain loss functions such as that which is used in regression and that which exists in SVM is reviewed and the ideas behind when to apply different types of loss functions is explored. The pinball loss, which is an important part of the generalized loss function proposed, is then presented to show that changing the penalty conditioned upon the sign of the residual creates quantile estimates, a result that can be useful in certain applications. After laying out the background for a generalized asymmetric loss, the proposed loss function is then derived and shown to have a close form solution within an SVM context. How this generalized SVM works is then shown graphically to display how the method works. The asymmetric SVM is applied to various datasets to show how it can benefit practical problems. In addition, combinations of these SVMs are introduced to demonstrate the ability for better performance when applied in an ensemble framework. 

\subsection{Asymmetric Random Forest Variable Selection (Chapter~\ref{chapter:AsymRFpaper})}
Little has been done to use asymmetric learning in the area of variable selection. Variable selection algorithms are mostly design to find variables that can predict all possible values of the independent variable. Take for instance a dependent variable with a positive and negative class. Variable selection techniques are designed to pick variables that are predictive of when an observation is negative and when an observation is positive. In these techniques, there is little value for picking variables that are able to predict one class accurately. As a result, variables that are only able to predict a positive class accurately will not be selected when using traditional algorithms. Chapter~\ref{chapter:AsymRFpaper} explores the use of asymmetry by using ensemble based methods and in particular, random forests, to discover variables that are asymmetrically predictive. 


This chapter modifies the models for variable selection such that it is more attuned to a specific class of the dependent variable. This is done by modifying the loss function within the tree. In trees, the loss function is actually the purity function which comes in the form of the Gini index or entropy. In this research, we will focus on modifying the Gini index such that purity of one class is more desired.

We plan to further explore how we can modify the random forest to provide for better outcomes in asymmetric variable selection. Recoding of a random forest is currently being done so that we can modify the algorithm to be used for experiments. Simulated datasets that has variables that are predictive in both direction, predictive in only 1 direction, and not predictive at all will be used as a test to see if the modification works. After which, practical data will be used to test if the method is useful is real applications. The modified random forest will be tested against options exercise and sell data which is known to be asymmetric in nature and the results will be reported. 

\subsection{Random Forest with Non-linear Stratified Data (Chapter~\ref{chapter:RFStratum})}
Stratification, or blocking is commonly used in studies to reduce the variability of nuisance parameters from the model. In these experiments it is commonly assumed that the nuisance parameter affects the response in a linear way and a stratification parameter added to the model will be able to account for this nuisance variability in the model. This method of variability reduction falls short when the relationship between the response and predictors changes between stratums. As a result, the relationship between the predictor and the responses is non-linear in the entire predictor space but linear upon conditioning on the subspace where the observation lies. To find such subspaces or stratums, we attempt to use random forest to decipher such spaces by delineating clusters within the space. We apply this technique to matched data which adds an extra level of complexity in that the data is dependent. Each observation is either a case or a control and the goal is to find the relationship between a case and control. Thus, in constructing the random forest solution, we have to be careful not to delink the case and control observations in the dataset. 

An algorithm for delineating stratums has between completed and some preliminary results have been produced. There is much to be done in optimizing the method by trying different parameters such as the number of trees in the forest and also different methods of measuring the effectiveness of using such a solution. After the method has been refined, we will then apply it to real life clinical data to see if it is able to give better insights to relationships between cases and controls. 

 